---

title: Introduction to MLE and MAP

keywords: fastai
sidebar: home_sidebar

summary: "Summary: Maximum Likelihood Estimation, Maximum a Posteriori Estimation, MLE, MAP"
description: "Summary: Maximum Likelihood Estimation, Maximum a Posteriori Estimation, MLE, MAP"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_mle_map.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maximum-Likelihood-Estimation-(MLE)">Maximum Likelihood Estimation (MLE)<a class="anchor-link" href="#Maximum-Likelihood-Estimation-(MLE)"> </a></h2><p>Maximum Likelihood Estimation (MLE) is a principle that estimates the parameters of a statistical model, which makes the observed data most probable. In other words, MLE maximizes the data likelihood.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Parameter-Estimation:-Estimating-the-Probability-of-Heads">Parameter Estimation: Estimating the Probability of Heads<a class="anchor-link" href="#Parameter-Estimation:-Estimating-the-Probability-of-Heads"> </a></h2><p>Let's assume we have a random variable $X$ representing a coin. We can estimate the probability that it will turn up heads ($X = 1$) or tails ($X = 0$).</p>
<blockquote><p>Task:Estimate the probability of heads $\theta = P(X = 1)$</p>
</blockquote>
<p>Evidently, if $P(X=1)=\theta$, then $P(X=0)=1-\theta$. Since we do not know the "true" probability of heads, i.e. $P(X=1) = \theta$, we will use $\hat\theta$ to refer to its estimate.</p>
<p><strong>Question:</strong> <em>What is the probability of $\theta = P(X=1)?$</em></p>
<p>In general, <em>Maximum Likelihood Estimation</em> principle asks to choose parameter $\theta$ that maximizes $P(Data|\theta)$, or in other words maximizes the probability of the observed data. We assume that $\theta$ belongs to the set $\Theta \subset \mathbb{R}^n$. Therefore,</p>
<p>{% raw %}
$$\hat\theta_{MLE} = \underset{\theta}{\arg\max}\ P(Data|\theta)$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In regards to our coin flip example, if we flip the coin repeatedly, we observe that:</p>
<ul>
<li>It turns up heads $\alpha_1$ times</li>
<li>It turns up tails $\alpha_0$ times</li>
</ul>
<p>Intuitively, we can estimate the $P(X=1)$ from our training data (number of tosses) as the fraction of flips that ends up heads:</p>
<p>{% raw %}
$$ P(X=1) = \frac{\alpha_1}{\alpha_1 + \alpha_0}$$
{% endraw %}</p>
<p>For instance, if we flip the coin 40 times, seeing 18 heads and 22 tails, then we can estimate that:</p>
<p>{% raw %}
$$\hat\theta = P(X=1) = \frac{18}{40} = 0.45$$
{% endraw %}</p>
<p>And if we flip it 5 times, observing 3 heads and 2 tails, then we have:</p>
<p>{% raw %}
$$\hat\theta = P(X=1) = \frac{3}{5} = 0.6$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-to-Calculate-MLE?">How to Calculate MLE?<a class="anchor-link" href="#How-to-Calculate-MLE?"> </a></h2><p>First step in calculating the maximum likelihood estimator $\hat\theta$ is to define $P(Data|\theta)$. If we flip the coin once, then $P(Data|\theta) = \theta$ if the flip results in heads and $P(Data|\theta) = 1 - \theta$, if the flips turns tails. If we observe $D = \{1,0,1,1,0\}$ by tossing the coin 5 times, assuming the flips are independent and identically distributed (i.i.d), then we have:</p>
<p>{% raw %}
$$P(Data|\theta) = \theta\cdot(1-\theta)\cdot\theta\cdot\theta\cdot(1-\theta) = \theta^3\cdot(1-\theta)^2$$
{% endraw %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In general, if we flip the coin $n$ times, observing $\alpha_H$ heads and $\alpha_T$ tails, then</p>
<p>{% raw %}
$$P(Data|\theta) = \theta^{\alpha_H}\cdot(1-\theta)^{\alpha_T}$$
{% endraw %}</p>
<p>The next step is to find the value of $\theta$ that maximizes the $P(Data|\theta)$. When finding the MLE, it is often easier to maximize the log-likelihood function since,</p>
<p>{% raw %}
$$\underset{\theta}{\arg\max} \log P(Data|\theta) = \underset{\theta}{\arg\max}\ P(Data|\theta)$$
{% endraw %}</p>
<p>Let's call $J(\theta) = \log P(Data|\theta)$. Thus, in order to find the value of the $\theta$ that maximizes the $J(\theta)$, we calculate the derivative of $J(\theta)$ with respect to $\theta$, set it to zero and solve for $\theta$.</p>
<p>{% raw %}
$$\frac{\partial J(\theta)}{\partial \theta} = \frac{\partial[\alpha_H \log \theta + \alpha_T \log (1-\theta)]}{\partial \theta}= \alpha_H \frac{1}{\theta} - \alpha_T \frac{1}{1-\theta} = 0$$
{% endraw %}</p>
<p>Solving this for $\theta$ gives, $$\hat{\theta} = \dfrac{\alpha_H}{\alpha_H + \alpha_T}$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Question:</strong> How good is this MLE estimation?</p>
<p>If I fliped the coin 5 times: 3 heads, 2 tails: $\hat{\theta}_{MLE}=\frac{3}{5}=0.6$. What if I flipped 30 heads and 35 tails?  $\hat{\theta}_{MLE}=\frac{30}{65}=0.46$</p>
<p>Which estimator should we trust more? Let's assume that the coin is a goverment minted coin, meaning it's a fair coin and is "close" to 50-50 chance of heads/tails. It tells us that $\theta$ should be more likely about $0.5$. Therefore, we cannot quite rely on the estimate $\hat{\theta} = 0.6$.</p>
<p>In general, if we have plenty of data, MLE works well, but if we have a few observations such as 5 coin flips, our estimates will be unreliable. This leads us to the second principle for estimating parameters. This principle allows us to integrate our prior assumptions along with observed data to develop our ultimate estimate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Map-a-Posteriori-Estimation-(MAP)">Map a Posteriori Estimation (MAP)<a class="anchor-link" href="#Map-a-Posteriori-Estimation-(MAP)"> </a></h2><p>Considering our coin flip example, we assume that the coin is a goverment minted coin, meaning the $\theta$ is close to $0.5$. What can do we now that we have prior knowledge? How can we estimate the probability of heads?</p>
<p>We have prior knowledge about the coin, but particularly, at the beginning we do not have enough coin flips to estimate the probability of heads ($\theta$). However, we can add a number of <em>imaginary</em> coin flips. For instance, if we add 10 imaginary heads and 10 imaginary tails before we even start the first flip, $\hat{\theta}=\frac{10}{20}=0.5$, which describes our prior knowledge about the coin. After the first flip, if it turns up head $\hat{\theta}=\frac{1+10}{1+10+10}=0.52$.</p>
<p>We can see that as the number of coin flips increases, our final estimate becomes better, but more importantly, when we don't have plenty of flips, our estimate is still reliable. The more confident we are about our prior assumptions, the higher number of imaginary flips we can consider. Thus, we have:</p>
<p>{% raw %}
$$\hat{\theta} = \dfrac{\alpha_H + \lambda_H}{(\alpha_H+\lambda_H) + (\alpha_T+\lambda_T)}$$
{% endraw %}</p>
<p>where $\lambda_H$ and $\lambda_T$ are imaginary (or virtual) heads and tails respectively.</p>
<h3 id="Bayesian-Approach">Bayesian Approach<a class="anchor-link" href="#Bayesian-Approach"> </a></h3><p>We choose a Bayesian approach, therefore, rather than estimating a single $\theta$, we obtain a distribution over possible values of $\theta$. Then, choose the value of $\theta$ that is most probable, given the observed data and prior belief.</p>
<p>We need Bayes rule to proceed.</p>
<p>Chain rule: $$P(X,Y)=P(X|Y)P(Y)=P(Y|X)P(X)$$</p>
<p>Bayes rule: $$P(X|Y)=\frac{P(Y|X)P(X)}{P(Y)}$$</p>
<p>Using the Bayes rule, we have: $$P(\theta|Data)=\frac{P(Data|\theta)P(\theta)}{P(Data)}$$</p>
<p>Or equivalently, $$P(\theta|Data)\propto P(Data|\theta)P(\theta)$$</p>
<p>We can get rid of $P(Data)$, because it's independent of the parameter $\theta$.</p>
<p>$P(\theta|Data)$ is called "posterior", $P(Data|\theta)$ is called "likelihood" and $P(\theta)$ is called "prior".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MLE-vs.-MAP">MLE vs. MAP<a class="anchor-link" href="#MLE-vs.-MAP"> </a></h2><p>1- Maximum Likelihood estimation (MLE):</p>
<ul>
<li>Choose value of $\theta$ that maximizes the probability of observed
data.
{% raw %}
$$\hat{\theta}_{MLE}=\underset{\theta}{\arg\max}\ P(Data|\theta)$$
{% endraw %}</li>
</ul>
<p>2- Maximum a posteriori (MAP) estimation:</p>
<ul>
<li>Choose value of $\theta$ that is most probable given observed data and
prior belief.
$$
\begin{aligned}
\hat{\theta}_{MAP}&amp;=\underset{\theta}{\arg\max}\ P(\theta|Data)\\
&amp;=\underset{\theta}{\arg\max}\ P(Data|\theta) P(\theta)
\end{aligned}
$$</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Map-Estimation-for-Binomial-Distribution">Map Estimation for Binomial Distribution<a class="anchor-link" href="#Map-Estimation-for-Binomial-Distribution"> </a></h2><p>Likelihood is Binomial:$P(Data|\theta)={n\choose \alpha_H}\theta^{\alpha_H}(1-\theta)^{\alpha_T}$</p>
<p>If we assume prior is Beta distribution: $P(\theta)=\frac{\theta^{\beta_H-1}(1-\theta)^\beta_T-1}{B(\beta_H,\beta_T)}\sim Beta(\beta_H, \beta_T)$</p>
<p>$B(x,y)=\int_o^1 t^{x-1}(1-t)^{y-1}dt$</p>
<p>Then, posterior is Beta distribution: $P(\theta|Data)\sim Beta(\beta_H+\alpha_H, \beta_T+\alpha_T)$</p>
<p>And,</p>
$$
\begin{aligned}
\hat{\theta}_{MAP}&amp;=\underset{\theta}{\arg\max}\ P(Data|\theta) P(\theta)\\
&amp;=\frac{\alpha_H+\beta_H -1}{\alpha_H+\beta_H+\alpha_T+\beta_T -2}
\end{aligned}
$$<ul>
<li><p><strong>Conjugate prior:</strong> $P(\theta)$ is the conjugate prior for likelihood function $P(\theta|Data)$ if $P(\theta)$ and $P(\theta|Data)$ have the same form.</p>
</li>
<li><p>Beta prior is equivalent to extra coin flips</p>
</li>
<li><p>As the number of samples (e.g. coin flips) increases, the effect of prior is "washed out". It means as $N\rightarrow \infty$, prior is "forgotten".</p>
</li>
<li><p>For small sample size, prior is important.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implemented-Functions">Implemented Functions<a class="anchor-link" href="#Implemented-Functions"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="flip_coin" class="doc_header"><code>flip_coin</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/mle_map.py#L11" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>flip_coin</code>(<strong><code>num_of_experiments</code></strong>=<em><code>1000</code></em>, <strong><code>num_of_flips</code></strong>=<em><code>30</code></em>)</p>
</blockquote>
<p>Flip the coin <code>num_of_flips</code> times and repeat this experiment <code>num_of_experiments</code> times. And
return the number of heads grouped together in all the experiments.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># initialize the variables</span>
<span class="n">num_of_flips</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">num_of_experiments</span> <span class="o">=</span> <span class="mi">3000</span>
<span class="n">head_counts</span> <span class="o">=</span> <span class="n">flip_coin</span><span class="p">(</span><span class="n">num_of_experiments</span><span class="p">,</span><span class="n">num_of_flips</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's plot a chart and see the the number of heads in 30 flips in all the experiments.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_of_flips</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">source</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
  <span class="s1">&#39;Number of Heads&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
  <span class="s1">&#39;Number of Ways&#39;</span><span class="p">:</span> <span class="n">head_counts</span>
<span class="p">})</span>
<span class="c1">#     Bar chart</span>
<span class="n">bar_chart</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="o">.</span><span class="n">mark_bar</span><span class="p">()</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Number of Heads&#39;</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Number of Ways&#39;</span><span class="p">,</span>
<span class="p">)</span><span class="o">.</span><span class="n">properties</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Distribution of Heads&#39;</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">360</span><span class="p">)</span>
<span class="n">bar_chart</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

<div id="altair-viz-10142b9ecd3f4c34bde38391ed6390dd"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    const outputDiv = document.getElementById("altair-viz-10142b9ecd3f4c34bde38391ed6390dd");
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "data": {"name": "data-c7ad18303904753003e679bfb19caa8f"}, "mark": "bar", "encoding": {"x": {"type": "quantitative", "field": "Number of Heads"}, "y": {"type": "quantitative", "field": "Number of Ways"}}, "title": "Distribution of Heads", "width": 360, "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json", "datasets": {"data-c7ad18303904753003e679bfb19caa8f": [{"Number of Heads": 0, "Number of Ways": 0}, {"Number of Heads": 1, "Number of Ways": 0}, {"Number of Heads": 2, "Number of Ways": 0}, {"Number of Heads": 3, "Number of Ways": 0}, {"Number of Heads": 4, "Number of Ways": 0}, {"Number of Heads": 5, "Number of Ways": 0}, {"Number of Heads": 6, "Number of Ways": 0}, {"Number of Heads": 7, "Number of Ways": 0}, {"Number of Heads": 8, "Number of Ways": 0}, {"Number of Heads": 9, "Number of Ways": 0}, {"Number of Heads": 10, "Number of Ways": 2}, {"Number of Heads": 11, "Number of Ways": 9}, {"Number of Heads": 12, "Number of Ways": 16}, {"Number of Heads": 13, "Number of Ways": 28}, {"Number of Heads": 14, "Number of Ways": 54}, {"Number of Heads": 15, "Number of Ways": 124}, {"Number of Heads": 16, "Number of Ways": 174}, {"Number of Heads": 17, "Number of Ways": 257}, {"Number of Heads": 18, "Number of Ways": 326}, {"Number of Heads": 19, "Number of Ways": 379}, {"Number of Heads": 20, "Number of Ways": 351}, {"Number of Heads": 21, "Number of Ways": 351}, {"Number of Heads": 22, "Number of Ways": 289}, {"Number of Heads": 23, "Number of Ways": 249}, {"Number of Heads": 24, "Number of Ways": 156}, {"Number of Heads": 25, "Number of Ways": 109}, {"Number of Heads": 26, "Number of Ways": 68}, {"Number of Heads": 27, "Number of Ways": 36}, {"Number of Heads": 28, "Number of Ways": 15}, {"Number of Heads": 29, "Number of Ways": 5}, {"Number of Heads": 30, "Number of Ways": 0}, {"Number of Heads": 31, "Number of Ways": 2}, {"Number of Heads": 32, "Number of Ways": 0}, {"Number of Heads": 33, "Number of Ways": 0}, {"Number of Heads": 34, "Number of Ways": 0}, {"Number of Heads": 35, "Number of Ways": 0}, {"Number of Heads": 36, "Number of Ways": 0}, {"Number of Heads": 37, "Number of Ways": 0}, {"Number of Heads": 38, "Number of Ways": 0}, {"Number of Heads": 39, "Number of Ways": 0}, {"Number of Heads": 40, "Number of Ways": 0}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What does the plot look like? Does the bell shape ring a bell?</p>
<p>Now, we plot a Normal distribution with mean = <code>number_of_flips</code> and standard deviation = $\sqrt{mean/2}$  and check how it looks like.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">num_of_flips</span><span class="p">)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">num_of_flips</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">stddev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mean</span><span class="p">,</span><span class="n">stddev</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
    <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span>
<span class="p">})</span>
<span class="n">normal_chart</span> <span class="o">=</span> <span class="n">alt</span><span class="o">.</span><span class="n">Chart</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">mark_line</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
    <span class="n">alt</span><span class="o">.</span><span class="n">X</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">),</span>
    <span class="n">y</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">properties</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Normal Distribution&#39;</span><span class="p">,</span><span class="n">width</span><span class="o">=</span><span class="mi">360</span><span class="p">)</span>
<span class="n">normal_chart</span> <span class="o">|</span> <span class="n">bar_chart</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">

<div id="altair-viz-10772a3e796f4a2bab5b9f8799ac775f"></div>
<script type="text/javascript">
  (function(spec, embedOpt){
    const outputDiv = document.getElementById("altair-viz-10772a3e796f4a2bab5b9f8799ac775f");
    const paths = {
      "vega": "https://cdn.jsdelivr.net/npm//vega@5?noext",
      "vega-lib": "https://cdn.jsdelivr.net/npm//vega-lib?noext",
      "vega-lite": "https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext",
      "vega-embed": "https://cdn.jsdelivr.net/npm//vega-embed@6?noext",
    };

    function loadScript(lib) {
      return new Promise(function(resolve, reject) {
        var s = document.createElement('script');
        s.src = paths[lib];
        s.async = true;
        s.onload = () => resolve(paths[lib]);
        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);
        document.getElementsByTagName("head")[0].appendChild(s);
      });
    }

    function showError(err) {
      outputDiv.innerHTML = `<div class="error" style="color:red;">${err}</div>`;
      throw err;
    }

    function displayChart(vegaEmbed) {
      vegaEmbed(outputDiv, spec, embedOpt)
        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));
    }

    if(typeof define === "function" && define.amd) {
      requirejs.config({paths});
      require(["vega-embed"], displayChart, err => showError(`Error loading script: ${err.message}`));
    } else if (typeof vegaEmbed === "function") {
      displayChart(vegaEmbed);
    } else {
      loadScript("vega")
        .then(() => loadScript("vega-lite"))
        .then(() => loadScript("vega-embed"))
        .catch(showError)
        .then(() => displayChart(vegaEmbed));
    }
  })({"config": {"view": {"continuousWidth": 400, "continuousHeight": 300}}, "hconcat": [{"data": {"name": "data-8b48908db5d271e2956e2a8272fc8e07"}, "mark": {"type": "line", "color": "red"}, "encoding": {"x": {"type": "quantitative", "field": "x"}, "y": {"type": "quantitative", "field": "y"}}, "title": "Normal Distribution", "width": 360}, {"data": {"name": "data-c7ad18303904753003e679bfb19caa8f"}, "mark": "bar", "encoding": {"x": {"type": "quantitative", "field": "Number of Heads"}, "y": {"type": "quantitative", "field": "Number of Ways"}}, "title": "Distribution of Heads", "width": 360}], "$schema": "https://vega.github.io/schema/vega-lite/v4.0.2.json", "datasets": {"data-8b48908db5d271e2956e2a8272fc8e07": [{"x": 0, "y": 2.600281868827203e-10}, {"x": 1, "y": 1.8276568877457265e-09}, {"x": 2, "y": 1.1623567955302993e-08}, {"x": 3, "y": 6.688901526032106e-08}, {"x": 4, "y": 3.4828975312041506e-07}, {"x": 5, "y": 1.6409567867287274e-06}, {"x": 6, "y": 6.995586696268014e-06}, {"x": 7, "y": 2.6984954724388374e-05}, {"x": 8, "y": 9.418674667969561e-05}, {"x": 9, "y": 0.000297459915550561}, {"x": 10, "y": 0.000850036660252035}, {"x": 11, "y": 0.0021979480031862693}, {"x": 12, "y": 0.005142422126351767}, {"x": 13, "y": 0.010886507726916078}, {"x": 14, "y": 0.02085355003628301}, {"x": 15, "y": 0.03614447853363626}, {"x": 16, "y": 0.05668582612248957}, {"x": 17, "y": 0.0804410163156249}, {"x": 18, "y": 0.10328830949345566}, {"x": 19, "y": 0.12000389484301362}, {"x": 20, "y": 0.126156626101008}, {"x": 21, "y": 0.12000389484301362}, {"x": 22, "y": 0.10328830949345566}, {"x": 23, "y": 0.0804410163156249}, {"x": 24, "y": 0.05668582612248957}, {"x": 25, "y": 0.03614447853363626}, {"x": 26, "y": 0.02085355003628301}, {"x": 27, "y": 0.010886507726916078}, {"x": 28, "y": 0.005142422126351767}, {"x": 29, "y": 0.0021979480031862693}, {"x": 30, "y": 0.000850036660252035}, {"x": 31, "y": 0.000297459915550561}, {"x": 32, "y": 9.418674667969561e-05}, {"x": 33, "y": 2.6984954724388374e-05}, {"x": 34, "y": 6.995586696268014e-06}, {"x": 35, "y": 1.6409567867287274e-06}, {"x": 36, "y": 3.4828975312041506e-07}, {"x": 37, "y": 6.688901526032106e-08}, {"x": 38, "y": 1.1623567955302993e-08}, {"x": 39, "y": 1.8276568877457265e-09}], "data-c7ad18303904753003e679bfb19caa8f": [{"Number of Heads": 0, "Number of Ways": 0}, {"Number of Heads": 1, "Number of Ways": 0}, {"Number of Heads": 2, "Number of Ways": 0}, {"Number of Heads": 3, "Number of Ways": 0}, {"Number of Heads": 4, "Number of Ways": 0}, {"Number of Heads": 5, "Number of Ways": 0}, {"Number of Heads": 6, "Number of Ways": 0}, {"Number of Heads": 7, "Number of Ways": 0}, {"Number of Heads": 8, "Number of Ways": 0}, {"Number of Heads": 9, "Number of Ways": 0}, {"Number of Heads": 10, "Number of Ways": 2}, {"Number of Heads": 11, "Number of Ways": 9}, {"Number of Heads": 12, "Number of Ways": 16}, {"Number of Heads": 13, "Number of Ways": 28}, {"Number of Heads": 14, "Number of Ways": 54}, {"Number of Heads": 15, "Number of Ways": 124}, {"Number of Heads": 16, "Number of Ways": 174}, {"Number of Heads": 17, "Number of Ways": 257}, {"Number of Heads": 18, "Number of Ways": 326}, {"Number of Heads": 19, "Number of Ways": 379}, {"Number of Heads": 20, "Number of Ways": 351}, {"Number of Heads": 21, "Number of Ways": 351}, {"Number of Heads": 22, "Number of Ways": 289}, {"Number of Heads": 23, "Number of Ways": 249}, {"Number of Heads": 24, "Number of Ways": 156}, {"Number of Heads": 25, "Number of Ways": 109}, {"Number of Heads": 26, "Number of Ways": 68}, {"Number of Heads": 27, "Number of Ways": 36}, {"Number of Heads": 28, "Number of Ways": 15}, {"Number of Heads": 29, "Number of Ways": 5}, {"Number of Heads": 30, "Number of Ways": 0}, {"Number of Heads": 31, "Number of Ways": 2}, {"Number of Heads": 32, "Number of Ways": 0}, {"Number of Heads": 33, "Number of Ways": 0}, {"Number of Heads": 34, "Number of Ways": 0}, {"Number of Heads": 35, "Number of Ways": 0}, {"Number of Heads": 36, "Number of Ways": 0}, {"Number of Heads": 37, "Number of Ways": 0}, {"Number of Heads": 38, "Number of Ways": 0}, {"Number of Heads": 39, "Number of Ways": 0}, {"Number of Heads": 40, "Number of Ways": 0}]}}, {"mode": "vega-lite"});
</script>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By comparing the charts we realize that</p>
<ul>
<li><p>As the sample size become larger, the distribution of samples approximate a normal distribution.</p>
</li>
<li><p>As we flip the coin repeatedly, number of heads and tails are getting equal because the number of heads has been peaked around 20 flips out of 40, which indicates that the probability of heads is close to $0.5$.</p>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><p>[1] Tom Mitchell <a href="http://www.cs.cmu.edu/~tom/mlbook/Joint_MLE_MAP.pdf">Estimating Probabilities</a></p>

</div>
</div>
</div>
</div>
 

