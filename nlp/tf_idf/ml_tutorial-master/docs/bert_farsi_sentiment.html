---

title: Sentiment Analysis with Multilingual Transformers

keywords: fastai
sidebar: home_sidebar

summary: "Summary: Transformers, BERT, Bert Tokenizer, Pretrained Models, Farsi Sentiment Analysis, Multiligual Transformers"
description: "Summary: Transformers, BERT, Bert Tokenizer, Pretrained Models, Farsi Sentiment Analysis, Multiligual Transformers"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 10_bert_farsi_sentiment.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Transformer</strong> revolves around the idea of a model that uses <em>attention</em> to increase the speed with which it can be trained. The primary motivation for designing a transformer was to enable parallel processing of the words in the sentences, i.e. to process the entire sentence at once. This parallel processing is not possible in LSTMs or RNNs or GRUs as they take words of the input sentence one by one. The first transformer was proposed in the paper <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>. There is a TensorFlow implementation of it is available <a href="https://github.com/tensorflow/tensor2tensor">here</a>. Also, Harvard’s NLP group provided a <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">guide</a> annotating the paper with PyTorch implementation.
These transformer models come in different shapes, sizes, and architectures and have their own ways of accepting input data: via <em>tokenization</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="BERT-Overview">BERT Overview<a class="anchor-link" href="#BERT-Overview"> </a></h2><p>BERT (Bidirectionnal Encoder Representations for Transformers) is a “new method of pre-training language representations” developed by Google in <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> and released in late 2018. Since it is pre-trained on generic large datasets (from Wikipedia and BooksCorpus), it can be used for a wide variety of NLP tasks like text classification, translation, summarization, and question answering. Here is the abstract from the paper:</p>
<p><em>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.</em></p>
<p>BERT provides pre-trained language models for English and 103 other languages that you can fine-tune to fit your needs. Fine-tuning a model means that we will slightly train it using our dataset on top of an already trained checkpoint. Here, we’ll see how to fine-tune the multilingual model to do sentiment analysis. To do that, we follow the steps below:</p>
<ol>
<li><p>Load and preprocess the data so that it can be used by the model.</p>
</li>
<li><p>Set-up a training loop using Keras' fit API; train the model on the training data</p>
</li>
<li><p>Evaluate the model on the testing data and compare to the actual results</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Sentiment-Analysis-on-Farsi-Text">Sentiment Analysis on Farsi Text<a class="anchor-link" href="#Sentiment-Analysis-on-Farsi-Text"> </a></h2><p>The following implementation shows how to use the <a href="https://huggingface.co/transformers/index.html">Transformers</a> library to obtain state-of-the-art results on the sequence classification task. This library "<em>provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between TensorFlow 2.0 and PyTorch</em>".</p>
<p>There are plenty of applications of classification on English text. Thus, the goal of my implementation is to perform sentiment classification on non-English text, in this case Farsi(Persian) language. As it takes time to run it on CPU, I created a Google Colab notebook version of the entire implementation, which you can access it <a href="https://colab.research.google.com/drive/12lM8FCqEQdCUCj1b52mN3Ggw85Mj0qYV?usp=sharing">here</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Load-the-data">Load the data<a class="anchor-link" href="#Load-the-data"> </a></h3><p>For this tutorial, I used a <a href="https://www.kaggle.com/saeedtqp/taaghche">Farsi dataset</a>, from Kaggle. It contains over 68000 comments about books gathered from a book website called <a href="https://taaghche.com/">taaghche</a>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>date</th>
      <th>comment</th>
      <th>bookname</th>
      <th>rate</th>
      <th>bookID</th>
      <th>like</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1395/11/14</td>
      <td>اسم کتاب   No one writes to the Colonel\nترجمش...</td>
      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1395/11/14</td>
      <td>طاقچه عزیز،نام کتاب"کسی به سرهنگ نامه نمینویسد...</td>
      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>
      <td>5.0</td>
      <td>3.0</td>
      <td>2.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1394/06/06</td>
      <td>بنظرم این اثر مارکز خیلی از صد سال تنهایی که ب...</td>
      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>
      <td>5.0</td>
      <td>3.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1393/09/02</td>
      <td>به نظر کتاب خوبی میومد اما من از ترجمش خوشم نی...</td>
      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>
      <td>2.0</td>
      <td>3.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1393/06/29</td>
      <td>کتاب خوبی است</td>
      <td>سرهنگ کسی ندارد برایش نامه بنویسد</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="load_data" class="doc_header"><code>load_data</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L22" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>load_data</code>(<strong><code>file_name</code></strong>)</p>
</blockquote>
<p>Read the CSV and creates a Panda dataframe fro the file content.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Initial-Data-Preprocessing">Initial Data Preprocessing<a class="anchor-link" href="#Initial-Data-Preprocessing"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Remove the unnecessary columns</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;bookname&#39;</span><span class="p">,</span> <span class="s1">&#39;bookID&#39;</span><span class="p">,</span> <span class="s1">&#39;like&#39;</span><span class="p">])</span>
<span class="c1"># df = df.rename(columns={&#39;Text&#39;:&#39;text&#39;,&#39;Suggestion&#39;: &#39;label&#39;})</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">rate</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;0&#39;</span>
<span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">rate</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rate&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>comment</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>اسم کتاب   No one writes to the Colonel\nترجمش...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>طاقچه عزیز،نام کتاب"کسی به سرهنگ نامه نمینویسد...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>بنظرم این اثر مارکز خیلی از صد سال تنهایی که ب...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>به نظر کتاب خوبی میومد اما من از ترجمش خوشم نی...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>کتاب خوبی است</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_pos</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="s1">&#39;1&#39;</span><span class="p">]</span>
<span class="n">df_neg</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="s1">&#39;0&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Postive examples: </span><span class="si">{}</span><span class="s1">  Negative examples: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df_pos</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">df_neg</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Postive examples: 55922  Negative examples: 13868
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Create-a-Balanced-Dataset">Create a Balanced Dataset<a class="anchor-link" href="#Create-a-Balanced-Dataset"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="create_small_dataset" class="doc_header"><code>create_small_dataset</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L36" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>create_small_dataset</code>(<strong><code>df_pos</code></strong>, <strong><code>df_neg</code></strong>, <strong><code>n_samples</code></strong>)</p>
</blockquote>
<p>Create a custom dataset of size <code>n_samples</code> from positive <code>df_pos</code> and negative <code>df-neg</code>
examples.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="remove_emoji" class="doc_header"><code>remove_emoji</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L72" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>remove_emoji</code>(<strong><code>text</code></strong>)</p>
</blockquote>
<p>Remove a number of emojis from text.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dataset_size</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">create_small_dataset</span><span class="p">(</span><span class="n">df_pos</span><span class="p">,</span> <span class="n">df_neg</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">)</span>

<span class="c1">#shuffle the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dataset</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>comment</th>
      <th>polarity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>اصلا خوب نبود</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>لطفا کتاب های بیشتری بزارید رشته حسابداری ممنو...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>با وجود اینکه ساده و روان نوشته شده و گوینده ه...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>بسیار عجیب و باورنکردنی</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>خوب نبود</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>9995</th>
      <td>خوبه</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9996</th>
      <td>سلام\nلطفا براش تخفیف بگزارید\nبهش احتیاج داریم!</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9997</th>
      <td>قیمتش حتی با تخفیف چهل درصد هم غیرمنطقیه برای ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9998</th>
      <td>افتضاح، بد، مزخرف</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9999</th>
      <td>فوق العادست</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>10000 rows × 2 columns</p>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;train data size: </span><span class="si">{}</span><span class="s1">    test data size: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train data size: 8000    test data size: 2000
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Convert-the-Data-into-Bert-Specific-Format">Convert the Data into Bert Specific Format<a class="anchor-link" href="#Convert-the-Data-into-Bert-Specific-Format"> </a></h2><p>We need to transform our data into a format BERT understands. This involves two steps. First, we create a list of <code>InputExample</code> objects using the constructor provided by Transformers library. Every <code>InputExample</code> must have the following structure:</p>
<ul>
<li><p><code>text_a</code> is the text we want to classify</p>
</li>
<li><p><code>text_b</code> is used if we're training a model to understand the relationship between sentences (i.e. is <code>text_b</code> a translation of <code>text_a</code>)? Is <code>text_b</code> an answer to the question asked by <code>text_a</code>?). This doesn't apply to our task, so we can leave <code>text_b</code> blank.</p>
</li>
<li><p><code>label</code> is the label of our example, i.e. True or False.</p>
</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="convert_data_into_input_example" class="doc_header"><code>convert_data_into_input_example</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L85" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>convert_data_into_input_example</code>(<strong><code>data</code></strong>)</p>
</blockquote>
<p>Covert the list of examples into a list of <code>InputExample</code> objects that is suitable
for BERT model.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_input_examples</span> <span class="o">=</span> <span class="n">convert_data_into_input_example</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
<span class="n">val_input_examples</span> <span class="o">=</span> <span class="n">convert_data_into_input_example</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we need to preprocess our data (i.e. <code>InputExample</code>'s) so that it matches the data BERT was trained on. Therefore, we need to do a few things:</p>
<ol>
<li><p>Lowercase our text (if we're using a BERT lowercase model)</p>
</li>
<li><p>Tokenize it (i.e. "sally says hi" -&gt; ["sally", "says", "hi"])</p>
</li>
<li><p>Break words into WordPieces (i.e. "calling" -&gt; ["call", "##ing"])</p>
</li>
<li><p>Map our words to indexes using a vocab file that BERT provides</p>
</li>
<li><p>Add special "CLS" and "SEP" tokens (see the <a href="https://github.com/google-research/bert">readme</a>)</p>
</li>
<li><p>Append "index" and "segment" tokens to each input (see the <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT paper</a>)</p>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tokenization">Tokenization<a class="anchor-link" href="#Tokenization"> </a></h2><p>Deep learning models accept certain kinds of inputs, which is vectors of integers, each value representing a token. Each string of text must first be converted to a list of indices to be fed to the model. The tokenizer takes care of that for us. We also need to add special tokens to the list of ids.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-multilingual-cased&#39;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;I liked that book very much!&#39;</span>
<span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
<span class="n">text_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenized_text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;text ids:&#39;</span><span class="p">,</span> <span class="n">text_ids</span><span class="p">)</span>
<span class="n">text_ids_with_special_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">build_inputs_with_special_tokens</span><span class="p">(</span><span class="n">text_ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;text ids with special tokens: &#39;</span><span class="p">,</span> <span class="n">text_ids_with_special_tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;I&#39;, &#39;like&#39;, &#39;##d&#39;, &#39;that&#39;, &#39;book&#39;, &#39;very&#39;, &#39;much&#39;, &#39;!&#39;]
text ids: [146, 11850, 10162, 10189, 12748, 12558, 13172, 106]
text ids with special tokens:  [101, 146, 11850, 10162, 10189, 12748, 12558, 13172, 106, 102]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Happily, there is a much simpler method that can do all the previous steps (i.e. tokenize, convert to indices and add special tokens) altogether.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">MAX_SEQ_LENGTH</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">encoded_bert_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">)</span>
<span class="c1"># encoded_bert_text = tokenizer.encode(text, add_special_tokens=True, max_length=MAX_SEQ_LENGTH, return_tensors=&#39;tf&#39;)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;encoded text: &#39;</span><span class="p">,</span> <span class="n">encoded_bert_text</span><span class="p">)</span>
<span class="n">decoded_text_with_special_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_bert_text</span><span class="p">)</span>
<span class="n">decoded_text_without_special_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_bert_text</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;decoded text with special token: &#39;</span><span class="p">,</span> <span class="n">decoded_text_with_special_token</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;decoded text without special token: &#39;</span><span class="p">,</span> <span class="n">decoded_text_without_special_token</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>encoded text:  [101, 146, 11850, 10162, 10189, 12748, 12558, 13172, 106, 102]
decoded text with special token:  [CLS] I liked that book very much! [SEP]
decoded text without special token:  I liked that book very much!
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, we still require other addtional information to deal and manage. Thankfully, the Transformer library has a method to directly convert a dataset of InputExamples into features BERT understands. This method is called <code>glue_convert_examples_to_features</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">label_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">]</span>

<span class="n">bert_train_dataset</span> <span class="o">=</span> <span class="n">glue_convert_examples_to_features</span><span class="p">(</span><span class="n">examples</span><span class="o">=</span><span class="n">train_input_examples</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s1">&#39;mrpc&#39;</span><span class="p">,</span> <span class="n">label_list</span><span class="o">=</span><span class="n">label_list</span><span class="p">)</span>
<span class="n">bert_val_dataset</span> <span class="o">=</span> <span class="n">glue_convert_examples_to_features</span><span class="p">(</span><span class="n">examples</span><span class="o">=</span><span class="n">val_input_examples</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s1">&#39;mrpc&#39;</span><span class="p">,</span> <span class="n">label_list</span><span class="o">=</span><span class="n">label_list</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
<span class="c1">#     print(&#39;Example: {}&#39;.format(bert_train_dataset[i]))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Example: {&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; Input_ids: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bert_train_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">input_ids</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; attention_mask: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bert_train_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; token_type_ids: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bert_train_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">token_type_ids</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; label: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bert_train_dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">label</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;}&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Example: {
 Input_ids: [101, 66847, 11892, 41002, 14495, 99500, 29869, 37951, 20208, 60230, 774, 92289, 11892, 10498, 763, 101420, 39387, 10700, 17197, 11626, 55532, 26973, 10700, 29869, 76528, 10700, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 label: 1
}
Example: {
 Input_ids: [101, 40160, 10429, 19015, 12455, 13141, 54731, 788, 71222, 54862, 80493, 10278, 26973, 10388, 756, 770, 11086, 10327, 53880, 54862, 10278, 756, 10289, 23155, 54862, 808, 17821, 19015, 55532, 29869, 86598, 10700, 10498, 12084, 10582, 10700, 789, 22929, 86598, 10700, 10498, 10641, 41883, 17821, 10327, 10700, 773, 36990, 26973, 10388, 12218, 24076, 756, 29869, 11626, 10388, 15511, 31688, 10582, 67125, 823, 24076, 756, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 label: 0
}
Example: {
 Input_ids: [101, 11800, 34353, 10700, 772, 22900, 29413, 763, 73478, 68625, 22955, 789, 53001, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 token_type_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
 label: 0
}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Let&#39;s take a look at one example from the dataset</span>
<span class="n">ex</span> <span class="o">=</span> <span class="n">bert_train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">in_ids</span> <span class="o">=</span> <span class="n">ex</span><span class="o">.</span><span class="n">input_ids</span>
<span class="n">decoded_sentence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">in_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoded_sentence</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>عالی بودحتما پیشنهاد میکنمانقدر زیبا بود که احساس میکردم فیلمش رو دارم میبینم
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Defining-the-Hyperparameters">Defining the Hyperparameters<a class="anchor-link" href="#Defining-the-Hyperparameters"> </a></h2><p>Before fine-tuning the model, we must define a few hyperparameters that will be used during the training such as the optimizer, the loss and the evaluation metric.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TFBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-multilingual-cased&#39;</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-08</span><span class="p">,</span> <span class="n">clipnorm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-the-Model">Training the Model<a class="anchor-link" href="#Training-the-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='For some reason, when passing the <code>bert_train_dataset</code>, (which is supposed to work), to the <code>model.fit()</code> did NOT work, so I had to workaround it.' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># This way did NOT work, so I had to workaround it.</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">bert_train_dataset</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">bert_val_dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Workaround">Workaround<a class="anchor-link" href="#Workaround"> </a></h2><p>I used the <code>bert_train_dataset</code> and created a list for each feature (i.e. <code>input_ids</code>, <code>attention_mask</code>, <code>token_type_ids</code> and <code>label</code>) and passed them as the arguments of the model. Please see transformers' <a href="https://huggingface.co/transformers/model_doc/bert.html">documentation</a> for more details.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="my_solution" class="doc_header"><code>my_solution</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L100" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>my_solution</code>(<strong><code>bdset</code></strong>)</p>
</blockquote>
<p>Create a list of input tensors required to be in the first argument of the
model call function for training. e.g. <code>model([input_ids, attention_mask, token_type_ids])</code>.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="example_to_features" class="doc_header"><code>example_to_features</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L117" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>example_to_features</code>(<strong><code>input_ids</code></strong>, <strong><code>attention_masks</code></strong>, <strong><code>token_type_ids</code></strong>, <strong><code>y</code></strong>)</p>
</blockquote>
<p>Convert a training example into the Bert compatible format.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-the-Model">Training the Model<a class="anchor-link" href="#Training-the-Model"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">my_solution</span><span class="p">(</span><span class="n">bert_train_dataset</span><span class="p">)</span>
<span class="n">x_val</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">my_solution</span><span class="p">(</span><span class="n">bert_val_dataset</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x_train shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;x_val shape: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x_val</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_train</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">y_train</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">example_to_features</span><span class="p">)</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">val_ds</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">x_val</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_val</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x_val</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">y_val</span><span class="p">))</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">example_to_features</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Format of model input examples: </span><span class="si">{}</span><span class="s1"> &#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_ds</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">val_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHS</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>x_train shape: (8000, 128)
x_val shape: (2000, 128)
Format of model input examples: &lt;TakeDataset shapes: ({input_ids: (None, 128), attention_mask: (None, 128), token_type_ids: (None, 128)}, (None, 1)), types: ({input_ids: tf.int64, attention_mask: tf.int64, token_type_ids: tf.int64}, tf.int64)&gt; 
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-Details">Training Details<a class="anchor-link" href="#Training-Details"> </a></h2>
<pre><code>Epoch 1/5
250/250 [==============================] - 137s 548ms/step - loss: 0.5838 - accuracy: 0.6894 - val_loss: 0.5079 - val_accuracy: 0.7665
Epoch 2/5
250/250 [==============================] - 134s 535ms/step - loss: 0.4867 - accuracy: 0.7747 - val_loss: 0.5002 - val_accuracy: 0.7650
Epoch 3/5
250/250 [==============================] - 134s 534ms/step - loss: 0.4131 - accuracy: 0.8227 - val_loss: 0.5112 - val_accuracy: 0.7685
Epoch 4/5
250/250 [==============================] - 134s 534ms/step - loss: 0.3515 - accuracy: 0.8558 - val_loss: 0.5692 - val_accuracy: 0.7630
Epoch 5/5
250/250 [==============================] - 134s 535ms/step - loss: 0.2901 - accuracy: 0.8871 - val_loss: 0.6147 - val_accuracy: 0.7575</code></pre>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="plot_history" class="doc_header"><code>plot_history</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L124" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>plot_history</code>(<strong><code>history</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Plot-the-Loss-and-Accuracy">Plot the Loss and Accuracy<a class="anchor-link" href="#Plot-the-Loss-and-Accuracy"> </a></h2><p>We can see the training and validation loss as well as accuracy in the plots below. Please note that after epoch 3 <code>validation loss</code> starts to grow, which means the model begins to overfit. Therefore 3 epochs would be enough and work better since it has better accuracy of <code>0.7685</code>.</p>
<p><img src="/ml_tutorial/images/bert_5_epochs_lr2-e5_sparse.png" alt=""></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">val_ds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="n">predictions_classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;comment: </span><span class="si">{}</span><span class="se">\n</span><span class="s1">, actual label: </span><span class="si">{}</span><span class="s1">, predicted label: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;comment&#39;</span><span class="p">],</span> <span class="n">val_input_examples</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">predictions_classes</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>comment: کسالت آور بود 
ولی چون یک داستان نسباتا واقعیست ، به خاطر حقایقی که در اون وجود داشت بخوبی ما رو با فرهنگ و اعتقادات مردم اون دوره انگلیس آشنا میکنه
داستان کشتی تایتانیک
, actual label: 0, predicted label: 1
comment: شیوایی کلام میتوانست بهتر باشد
, actual label: 0, predicted label: 0
comment: باسلام،برای قشرخاصی طراحی شده کتاب ودرکش برای عموم کمی سخت وگیج کننده است
, actual label: 0, predicted label: 1
comment: بد نبود- راضی کننده بود
, actual label: 1, predicted label: 0
comment: من به عنوان یه نویسنده این مجله تقاضا میکنم همه ازش حمایت کنند! این یه کار نمونه ای علمی ترویجی نو تو زمینه زبان کوردی هست و نیاز به حمایت همه داره
, actual label: 1, predicted label: 0
comment: دوستشدارمدوستش خواهی داشت✌
, actual label: 1, predicted label: 0
comment: خیلی قشنگ بود  
مرسی طاقچه جون
, actual label: 0, predicted label: 1
comment: همه کتاب داره درباره جزییات ظاهر بقیه و محیط صحبت میکنهچنگی ب دل نمیزنهبسیار خسته کنندس
, actual label: 0, predicted label: 0
comment: کتاب رو خوندم
به نظرم کتاب مفیدی میتونه باشه
فقط طیق معمول کتب روانشناسی داستان زیاد داره
خودم فقط قسمتهای غیر داستانی رو خوندم اگر ابهام ایجاد میشد داستانش رو هم میخوندم
کلا خوبه
حداقل متوجه میشید زبان عشق خودتون چیه و ب طرف مقابلتون میتوتید بگید با من اینطوری باش 
, actual label: 1, predicted label: 1
comment: کتاب با بررسی وقایع کربلا و تطبیق آنها با زندگی امروز به ما هشدار می‌دهد تا به سمت همان فرهنگی که فرزند رسول خدا را به شهادت رساند حرکت نکنیم و این کار را با دلایل منطقی و بسیار هوشمندانه انجام داده است
پیشنهاد میکنم این کتاب رو حتماً بخونید چون جواب بسیاری از سوالاتتون رو خواهید گرفت
, actual label: 1, predicted label: 1</code></pre>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="example_to_features_predict" class="doc_header"><code>example_to_features_predict</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L144" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>example_to_features_predict</code>(<strong><code>input_ids</code></strong>, <strong><code>attention_masks</code></strong>, <strong><code>token_type_ids</code></strong>)</p>
</blockquote>
<p>Convert the test examples into Bert compatible format.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="get_prediction" class="doc_header"><code>get_prediction</code><a href="https://github.com/sci2lab/ml_tutorial/tree/master/ml_tutorial/bert_farsi.py#L153" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>get_prediction</code>(<strong><code>in_sentences</code></strong>)</p>
</blockquote>
<p>Prepare the test comments and return the predictions.</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># pred_sentences = [&#39;به نظر کتاب خوبی نمی ومد&#39;, &#39; رایگان بودنش فوق العادش میکند&#39;]</span>
<span class="n">pred_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">comment</span> <span class="k">for</span> <span class="n">comment</span><span class="p">,</span><span class="n">l</span> <span class="ow">in</span> <span class="n">test_data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">get_prediction</span><span class="p">(</span><span class="n">pred_sentences</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>('واقعا چطور همچین کتابایی اجازه نشر پیدا میکنن؟', 0)
('روش چی ؟', 0)
('بنظرم کتاب زندگینامه شهید رو رایگان کردن صورت خوشی نداره! چون اونی که براش مهمه بخونه پولشو میده میخره کسی هم که سلیقه مطالعه کتابش این مدلی نیست، رایگانم بشه نمیخونه', 0)
('سوالات کجا میاد کجا باید جواب بدیم', 0)
('سمنوپزان ، مسلول و  زن زیادی  خوب بودند ؛ در کل ادم رو به گذشته های دور و شیوه زندگی اونها میبره و از این جهت فوق العاده است\nممنون از طاقچه بابت برنامه خوبش و به خاطر کتابهایی که رایگان کردید', 1)
('کتاب فوق العاده ای بود به خیلی از شبهات در غالب داستان جواب داده  حتما پیشنهاد میکنم', 1)
('چقدر قشنگ بالا و پایین زندگی رو نشون داده بود  ', 0)
('ترجمه اش اصلا خوب نیست', 0)
('واقعا این کتاب عالییییییییییییییییییییییییییییییییییییییییییییییییه یه حرف نگفته اس که غمباد شده تو دلمون واقعا دست مریزاد سرکاره خانوم عالیشاهی با آرزوی بهترین ها و بالاترین درجات برای شما نویسنده محترمه', 1)
('واقعاً قیمت ها رو بر چه اساس تعیین میکنید نمیدونم قیمت نسخه چاپی ۵۰ تومان هست که با توجه به قیمت کاغذ منطقی هست اما نسخه الکترونیک برای یک کتاب نباید انقدر گران باشد بنظرم طاقچه باید محدودیت های مشخص شده قرار بده برای قیمت', 0)
('واقعا صداش خوب نیست چون انگار داره برا بچه ها قصه میگه همچین متن هایی رو باید یه نفر با صدای قوی ومحکم وبا جذبه قرائت کنه ', 0)
('خیلی داستان جذابی نداشت و در آخر داستان هم\u200c اسلام هراسی بیداد میکرد', 0)
('سلام من این کتاب را قبلا خریداری کردم نمی دانم چرا الان فایل کتاب را باز نمی کند ؟', 0)
('آیا حرف دلواپسان درست نبود؟', 0)
('عالی', 1)
('کتاب سطح پایین با نثری درهم پیچیده البته ترجمه نامناسب', 0)
('قشنگ بود', 1)
('ترجمه خیلی خیلی خیلی بد و غیرقابل فهم', 0)
('یکی از بهترین و ساده ترین کتاب ها برای کسایی که دنبال راه حلی هستند تا در زندگی مالی موفق شوند', 1)
('خیلی عالی بود لذت بردم امیدوارم شما هم فراموش نکنید اولین نفری بودم که به ازادیش فکر کردم به درخت، به ماهی، به دلیران تنگسیر، به پایانی خوش', 1)</code></pre>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Evaluating the BERT model&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">val_ds</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

<pre><code>Evaluating the BERT model
32/32 [==============================] - 9s 293ms/step - loss: 0.6147 - accuracy: 0.7575
[0.6146795749664307, 0.7574999928474426]</code></pre>

</div>
</div>
</div>
</div>
 

