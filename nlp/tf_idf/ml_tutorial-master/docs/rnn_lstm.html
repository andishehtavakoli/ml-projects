---

title: Title

keywords: fastai
sidebar: home_sidebar

summary: "summary"
description: "summary"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 07_rnn_lstm.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
    {% raw %}
        
<div class="cell border-box-sizing code_cell rendered">

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># https://github.com/keras-team/keras/blob/master/examples/addition_rnn.py</span>
<span class="c1"># #hide</span>
<span class="c1"># import tensorflow as tf</span>
<span class="c1"># from tensorflow import keras</span>
<span class="c1"># from tensorflow.keras.models import Sequential</span>
<span class="c1"># from tensorflow.keras import layers</span>
<span class="c1"># import numpy as np</span>
<span class="c1"># from six.moves import range</span>


<span class="c1"># class CharacterTable(object):</span>
<span class="c1">#     &quot;&quot;&quot;Given a set of characters:</span>
<span class="c1">#     + Encode them to a one-hot integer representation</span>
<span class="c1">#     + Decode the one-hot or integer representation to their character output</span>
<span class="c1">#     + Decode a vector of probabilities to their character output</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     def __init__(self, chars):</span>
<span class="c1">#         &quot;&quot;&quot;Initialize character table.</span>
<span class="c1">#         # Arguments</span>
<span class="c1">#             chars: Characters that can appear in the input.</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         self.chars = sorted(set(chars))</span>
<span class="c1">#         self.char_indices = dict((c, i) for i, c in enumerate(self.chars))</span>
<span class="c1">#         self.indices_char = dict((i, c) for i, c in enumerate(self.chars))</span>

<span class="c1">#     def encode(self, C, num_rows):</span>
<span class="c1">#         &quot;&quot;&quot;One-hot encode given string C.</span>
<span class="c1">#         # Arguments</span>
<span class="c1">#             C: string, to be encoded.</span>
<span class="c1">#             num_rows: Number of rows in the returned one-hot encoding. This is</span>
<span class="c1">#                 used to keep the # of rows for each data the same.</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         x = np.zeros((num_rows, len(self.chars)))</span>
<span class="c1">#         for i, c in enumerate(C):</span>
<span class="c1">#             x[i, self.char_indices[c]] = 1</span>
<span class="c1">#         return x</span>

<span class="c1">#     def decode(self, x, calc_argmax=True):</span>
<span class="c1">#         &quot;&quot;&quot;Decode the given vector or 2D array to their character output.</span>
<span class="c1">#         # Arguments</span>
<span class="c1">#             x: A vector or a 2D array of probabilities or one-hot representations;</span>
<span class="c1">#                 or a vector of character indices (used with `calc_argmax=False`).</span>
<span class="c1">#             calc_argmax: Whether to find the character index with maximum</span>
<span class="c1">#                 probability, defaults to `True`.</span>
<span class="c1">#         &quot;&quot;&quot;</span>
<span class="c1">#         if calc_argmax:</span>
<span class="c1">#             x = x.argmax(axis=-1)</span>
<span class="c1">#         return &#39;&#39;.join(self.indices_char[x] for x in x)</span>


<span class="c1"># class colors:</span>
<span class="c1">#     ok = &#39;\033[92m&#39;</span>
<span class="c1">#     fail = &#39;\033[91m&#39;</span>
<span class="c1">#     close = &#39;\033[0m&#39;</span>

<span class="c1"># # Parameters for the model and dataset.</span>
<span class="c1"># TRAINING_SIZE = 50000</span>
<span class="c1"># DIGITS = 3</span>
<span class="c1"># REVERSE = True</span>

<span class="c1"># # Maximum length of input is &#39;int + int&#39; (e.g., &#39;345+678&#39;). Maximum length of</span>
<span class="c1"># # int is DIGITS.</span>
<span class="c1"># MAXLEN = DIGITS + 1 + DIGITS</span>

<span class="c1"># # All the numbers, plus sign and space for padding.</span>
<span class="c1"># chars = &#39;0123456789+ &#39;</span>
<span class="c1"># ctable = CharacterTable(chars)</span>

<span class="c1"># questions = []</span>
<span class="c1"># expected = []</span>
<span class="c1"># seen = set()</span>
<span class="c1"># print(&#39;Generating data...&#39;)</span>
<span class="c1"># while len(questions) &lt; TRAINING_SIZE:</span>
<span class="c1">#     f = lambda: int(&#39;&#39;.join(np.random.choice(list(&#39;0123456789&#39;))</span>
<span class="c1">#                     for i in range(np.random.randint(1, DIGITS + 1))))</span>
<span class="c1">#     a, b = f(), f()</span>
<span class="c1">#     # Skip any addition questions we&#39;ve already seen</span>
<span class="c1">#     # Also skip any such that x+Y == Y+x (hence the sorting).</span>
<span class="c1">#     key = tuple(sorted((a, b)))</span>
<span class="c1">#     if key in seen:</span>
<span class="c1">#         continue</span>
<span class="c1">#     seen.add(key)</span>
<span class="c1">#     # Pad the data with spaces such that it is always MAXLEN.</span>
<span class="c1">#     q = &#39;{}+{}&#39;.format(a, b)</span>
<span class="c1">#     query = q + &#39; &#39; * (MAXLEN - len(q))</span>
<span class="c1">#     ans = str(a + b)</span>
<span class="c1">#     # Answers can be of maximum size DIGITS + 1.</span>
<span class="c1">#     ans += &#39; &#39; * (DIGITS + 1 - len(ans))</span>
<span class="c1">#     if REVERSE:</span>
<span class="c1">#         # Reverse the query, e.g., &#39;12+345  &#39; becomes &#39;  543+21&#39;. (Note the</span>
<span class="c1">#         # space used for padding.)</span>
<span class="c1">#         query = query[::-1]</span>
<span class="c1">#     questions.append(query)</span>
<span class="c1">#     expected.append(ans)</span>
<span class="c1"># print(&#39;Total addition questions:&#39;, len(questions))</span>

<span class="c1"># print(&#39;Vectorization...&#39;)</span>
<span class="c1"># x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)</span>
<span class="c1"># y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)</span>
<span class="c1"># for i, sentence in enumerate(questions):</span>
<span class="c1">#     x[i] = ctable.encode(sentence, MAXLEN)</span>
<span class="c1"># for i, sentence in enumerate(expected):</span>
<span class="c1">#     y[i] = ctable.encode(sentence, DIGITS + 1)</span>

<span class="c1"># # Shuffle (x, y) in unison as the later parts of x will almost all be larger</span>
<span class="c1"># # digits.</span>
<span class="c1"># indices = np.arange(len(y))</span>
<span class="c1"># np.random.shuffle(indices)</span>
<span class="c1"># x = x[indices]</span>
<span class="c1"># y = y[indices]</span>

<span class="c1"># # Explicitly set apart 10% for validation data that we never train over.</span>
<span class="c1"># split_at = len(x) - len(x) // 10</span>
<span class="c1"># (x_train, x_val) = x[:split_at], x[split_at:]</span>
<span class="c1"># (y_train, y_val) = y[:split_at], y[split_at:]</span>

<span class="c1"># print(&#39;Training Data:&#39;)</span>
<span class="c1"># print(x_train.shape)</span>
<span class="c1"># print(y_train.shape)</span>

<span class="c1"># print(&#39;Validation Data:&#39;)</span>
<span class="c1"># print(x_val.shape)</span>
<span class="c1"># print(y_val.shape)</span>

<span class="c1"># # Try replacing GRU, or SimpleRNN.</span>
<span class="c1"># RNN = layers.LSTM</span>
<span class="c1"># HIDDEN_SIZE = 128</span>
<span class="c1"># BATCH_SIZE = 128</span>
<span class="c1"># LAYERS = 1</span>

<span class="c1"># print(&#39;Build model...&#39;)</span>
<span class="c1"># model = Sequential()</span>
<span class="c1"># # &quot;Encode&quot; the input sequence using an RNN, producing an output of HIDDEN_SIZE.</span>
<span class="c1"># # Note: In a situation where your input sequences have a variable length,</span>
<span class="c1"># # use input_shape=(None, num_feature).</span>
<span class="c1"># model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))</span>
<span class="c1"># # As the decoder RNN&#39;s input, repeatedly provide with the last output of</span>
<span class="c1"># # RNN for each time step. Repeat &#39;DIGITS + 1&#39; times as that&#39;s the maximum</span>
<span class="c1"># # length of output, e.g., when DIGITS=3, max output is 999+999=1998.</span>
<span class="c1"># model.add(layers.RepeatVector(DIGITS + 1))</span>
<span class="c1"># # The decoder RNN could be multiple layers stacked or a single layer.</span>
<span class="c1"># for _ in range(LAYERS):</span>
<span class="c1">#     # By setting return_sequences to True, return not only the last output but</span>
<span class="c1">#     # all the outputs so far in the form of (num_samples, timesteps,</span>
<span class="c1">#     # output_dim). This is necessary as TimeDistributed in the below expects</span>
<span class="c1">#     # the first dimension to be the timesteps.</span>
<span class="c1">#     model.add(RNN(HIDDEN_SIZE, return_sequences=True))</span>

<span class="c1"># # Apply a dense layer to the every temporal slice of an input. For each of step</span>
<span class="c1"># # of the output sequence, decide which character should be chosen.</span>
<span class="c1"># model.add(layers.TimeDistributed(layers.Dense(len(chars), activation=&#39;softmax&#39;)))</span>
<span class="c1"># model.compile(loss=&#39;categorical_crossentropy&#39;,</span>
<span class="c1">#               optimizer=&#39;adam&#39;,</span>
<span class="c1">#               metrics=[&#39;accuracy&#39;])</span>
<span class="c1"># model.summary()</span>

<span class="c1"># # Train the model each generation and show predictions against the validation</span>
<span class="c1"># # dataset.</span>
<span class="c1"># for iteration in range(1, 1):</span>
<span class="c1">#     print()</span>
<span class="c1">#     print(&#39;-&#39; * 50)</span>
<span class="c1">#     print(&#39;Iteration&#39;, iteration)</span>
<span class="c1">#     model.fit(x_train, y_train,</span>
<span class="c1">#               batch_size=BATCH_SIZE,</span>
<span class="c1">#               epochs=1,</span>
<span class="c1">#               validation_data=(x_val, y_val))</span>
<span class="c1">#     # Select 10 samples from the validation set at random so we can visualize</span>
<span class="c1">#     # errors.</span>
<span class="c1">#     for i in range(10):</span>
<span class="c1">#         ind = np.random.randint(0, len(x_val))</span>
<span class="c1">#         rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]</span>
<span class="c1">#         preds = model.predict_classes(rowx, verbose=0)</span>
<span class="c1">#         q = ctable.decode(rowx[0])</span>
<span class="c1">#         correct = ctable.decode(rowy[0])</span>
<span class="c1">#         guess = ctable.decode(preds[0], calc_argmax=False)</span>
<span class="c1">#         print(&#39;Q&#39;, q[::-1] if REVERSE else q, end=&#39; &#39;)</span>
<span class="c1">#         print(&#39;T&#39;, correct, end=&#39; &#39;)</span>
<span class="c1">#         if correct == guess:</span>
<span class="c1">#             print(colors.ok + &#39;☑&#39; + colors.close, end=&#39; &#39;)</span>
<span class="c1">#         else:</span>
<span class="c1">#             print(colors.fail + &#39;☒&#39; + colors.close, end=&#39; &#39;)</span>
<span class="c1">#         print(guess)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}
</div>
 

