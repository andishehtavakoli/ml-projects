# AUTOGENERATED! DO NOT EDIT! File to edit: 05_neural_network.ipynb (unless otherwise specified).

__all__ = ['NeuralNetwork']

# Cell
import numpy as np
import pandas as pd
import altair as alt
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_circles, make_moons, make_classification
from sklearn.preprocessing import normalize



class NeuralNetwork:
    """
    Implements a 2-layer neural network from scratch.
    """

    def __init__(self,x,y,n_hidden_units=10,learning_rate=0.01,reg_lambda=0):
        self.x = x
        self.y = y
        self.y_hat = np.zeros(y.shape)
        self.n_hidden_units = n_hidden_units
        self.W1 = np.random.randn(x.shape[1],n_hidden_units)
        self.W2 = np.random.randn(n_hidden_units,1)
        self.b1 = np.zeros((1,n_hidden_units))
        self.b2 = np.zeros((1,1))
        self.learning_rate = learning_rate
        self.epsilon = 1e-4
        self.reg_lambda = reg_lambda
        self.cost_history = []


    def sigmoid(self,x):
        """
        Computes the Sigmoid activation of `x`.
        """
        return 1 / (1 + np.exp(-x))


    def sigmoid_prime(self,x):
        """
        Computes the derivative of Sigmoid function for `x`.
        """
        return x * (1 - x)


    def feedforward(self):
        """
        Performs a forward pass through the model.
        """

        z1 = np.dot(self.x,self.W1) + self.b1
        self.a1 = self.sigmoid(z1)
        z2 = np.dot(self.a1,self.W2) + self.b2
        self.y_hat  = self.sigmoid(z2)


    def cross_entropy(self):
        """
        Computes the loss of the model with l2 regularization.
        """
        regularization = self.reg_lambda * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)))
        loss = -np.mean(self.y * np.log(self.y_hat + self.epsilon) + (1 - self.y) * np.log(1 - self.y_hat + self.epsilon)) + regularization
        return loss


    def backprop(self):
        """
        Performs a backpropagation and updates the weights.
        """
        delta_2 = self.y_hat - self.y
        dW2 = np.dot(self.a1.T,delta_2) + self.reg_lambda * self.W2
        db2 = delta_2
        z2_delta = np.dot(delta_2,self.W2.T)
        delta_1 = z2_delta * self.sigmoid_prime(self.a1)
        dW1 = np.dot(self.x.T,delta_1) + self.reg_lambda * self.W1
        db1 = delta_1

        self.W2 = self.W2 - self.learning_rate * dW2
        self.b2 = self.b2 - self.learning_rate * np.sum(db2,axis=0)
        self.W1 = self.W1 - self.learning_rate * dW1
        self.b1 = self.b1 - self.learning_rate * np.sum(db1,axis=0)


    def train(self):
        """
        Starts training the model.
        """
        self.feedforward()
        loss = self.cross_entropy()
        self.cost_history.append(loss)
        self.backprop()

    def predict(self,data):
        """
        Computes the predictions on the `data`.
        """
        self.x = data
        self.feedforward()
        return self.y_hat


    def plot_cost(self):

        """
        Plots the cost vs iterations
        """
        xx = np.arange(n_iterations)
        source = pd.DataFrame({'x':xx, 'y':self.cost_history})
        chart = alt.Chart(source).mark_line().encode(
            alt.X("x",title="Iterations"),
            alt.Y("y",title="Cost")
        )
        return chart


    def plot_decision_boundary(self,x,y):
        """
        Plots the data along with decision boundary.
        """

        orig = pd.DataFrame({"x1":x[:,0].ravel(), "x2": x[:,1],"y":y.ravel()})
        ch_orig = alt.Chart(orig).mark_point().encode(
            alt.X("x1"),
            alt.Y("x2"),
            alt.Color("y:O",scale=alt.Scale(range=["red","blue"]),legend=None)
        ).properties(
            title="Original Data"
        )
        h = 0.04
        x_min, x_max = x[:, 0].min(), x[:, 0].max()
        y_min, y_max = x[:, 1].min(), x[:, 1].max()
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                                 np.arange(y_min, y_max, h))
        zz = np.c_[xx.ravel(),yy.ravel()]
        predictions = self.predict(zz)
        converted_predictions = np.asarray([1 if label > .5 else 0 for label in predictions])

        source = pd.DataFrame({"x":xx.ravel(),"y":yy.ravel(),"z":converted_predictions.ravel()})
        alt.data_transformers.disable_max_rows()
        decision_boundary = alt.Chart(source).mark_point(size=100,filled=True).encode(
            alt.X("x",title="x1"),
            alt.Y("y",title="x2"),
            alt.Color("z:O",scale=alt.Scale(range=['#eb726a','#6ab1eb']))
        #     alt.Opacity("z:Q",scale=alt.Scale(range=[0.1,1]))
        ).properties(
            title="Decision Boundary"
        )

        return ch_orig | alt.layer(decision_boundary,ch_orig).resolve_scale(color='independent')



    def run(self):

        """
        Starts the neural network.
        """
        for i in range(1,n_iterations + 1):
            self.train()
            if i % 1000 == 0:
                print("Iteration: {}".format(i))
                loss = self.cross_entropy()
                print("loss: {}".format(loss))
        cost = np.asarray(self.cost_history)
        print("Hidden Units = {}, Regularization Lambda = {}, Learning Rate = {}".format(self.n_hidden_units,self.reg_lambda,self.learning_rate))
