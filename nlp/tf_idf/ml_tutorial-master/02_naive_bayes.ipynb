{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "# all_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "> Summary: Naive Bayes, Text classification, Sentiment analysis, bag-of-words, BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Naive Bayes Method?\n",
    "\n",
    "Naive Bayes technique is a supervised method. It is a probabilistic learning method for classifying documents particularly text documents. It works based on the Naive Bayes assumption. Navie Bayes assumes that features $x_1, x_2, \\cdots, x_n$ are **conditionally independent** given the class labels $y$. In other words:\n",
    "\n",
    "$$P(x_1,x_2,\\cdots,x_n|y)=P(x_1|y)P(x_2|y)\\cdots P(x_n|y)=\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "Although often times $x_i$'s are not really conditionally independent in real world, nevertheless, the approach performs surprisingly well in practice, particularly document classification and spam filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (NB) for Text Classification\n",
    "\n",
    "Let $D$ be a corpus of documents where each document $d$ is represented by a bag of words (i.e. the order of words does not matter), and has a label $y_d$. If the total number of labels are two $y=\\{0,1\\}$, meaning that every document belongs to the class 0 or 1, then the problem is a binary classification. Otherwise it is called a multi-class classification, $y=\\{0,1, \\cdots, k\\}$.\n",
    "For instance, for sentiment analysis we have two classes: \"positive\" and \"negative\", therefore, $y=\\{0,1\\}$, 0 representing \"negative\" class and 1 representing \"positive\" class, respectively.\n",
    "\n",
    "The words of the documents are the features and the total number of features is the size of the vocabulary $|v|$ (i.e. total number of unique words in the corpus). For classifying documents, first we represent each document $d$ as a vector of the words $d=<x_{1}, x_{2},\\cdots,x_{v}>$. Then the probability of document $d$ being in class $y=c$ is:\n",
    "\n",
    "$$P(y=c|x_1,x_2,\\cdots,x_v)=\\dfrac{P(y=c)P(x_1,x_2,\\cdots,x_v|y=c)}{P(x_1,x_2,\\cdots,x_v)}$$\n",
    "\n",
    "Assuming conditional independence between $x_i$'s:\n",
    "\n",
    "$$P(y=c|x_1,x_2,\\cdots,x_v)=\\dfrac{P(y=c)\\prod_{i=1}^{|v|}P(x_i|y=c)}{P(x_1,x_2,\\cdots,x_v)}$$\n",
    "\n",
    "We can drop the denominator as it is a normalization constant. Thus we have:\n",
    "\n",
    "$$P(y=c|x_1,x_2,\\cdots,x_v)\\propto P(y=c)\\prod_{i=1}^{|v|}P(x_i|y=c)$$\n",
    "\n",
    "In text classification, our goal is to find the *best* class for the document $d$. The best class in NB classification is the most likely or *maximum a posteriori (MAP)* class $y^{map}$. Therefore:\n",
    "\n",
    "$$y^{map} =\\underset{y}{\\arg\\max}\\quad P(y=y_k)\\prod_i^{|v|} P(x_i|y=y_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Estimate Parameters $p(y)$ and $p(x_i|y)$\n",
    "In the context of text classification, to estimate the parameters $P(y)$ and $P(x_i|y)$ we use relative frequency, which assigns the most likely value of each parameter given the training data. For estimating the prior:\n",
    "\n",
    "$$\\hat{P}(y=c)=\\dfrac{N_c}{|D|}$$\n",
    "\n",
    "where $N_c$ is the total number of documents with label $c$ and $|D|$ is total number of documents in the corpus $D$.\n",
    "\n",
    "We estimate the conditional probability $\\hat{P}(x_i|y=c)$ as the relative frequency of term $x_i$ in documents belonging to class $c$:\n",
    "\n",
    "$$\\hat{P}(x_i=t|y=c)=\\dfrac{N_{ct} + \\alpha}{\\sum_{t'=1}^v (N_{ct'}+\\alpha)}=\\dfrac{N_{ct} + \\alpha}{\\sum_{t'=1}^v N_{ct'}+\\alpha|v|}$$\n",
    "\n",
    "where $N_{ct}$ is the number of occurrences of $t$ (i.e. count or frequency of $t$) in training documents from class $c$, and $\\sum_{t'=1}^v N_{ct'}$ is total count of all the words in documents with label $c$. The parameter $\\alpha \\geq 0$ is called the *smoothing prior*. You can think of it as \"virtual\" or \"imaginary\" counts, which prevents the probabilities from becomming zero when a feature does not exist in the training data. When $\\alpha =1$, it is called *Laplace smoothing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Example\n",
    "Let's assume that we have a small dataset shown below: (example is adopted from [here](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html))\n",
    "\n",
    "![](images/nb_toy_example.png)\n",
    "\n",
    "According to this dataset, we want to classify the test document. The vocabulary has 6 words: {\"Chinese\", \"Tokyo\", \"Japan\", \"Beijing\", \"Shanghai\", \"Macao\"}. Therefore, each document is a vector of these words. For example, document $d_1 = <2,0,0,1,0,0>$. Now, the first step to classify the test document $d_5$ is to calculate the priors:\n",
    "\n",
    "$\\hat{P}(y=yes)=\\frac{3}{4}$ and $\\hat{P}(y=no)=\\frac{1}{4}$. Then, we compute the conditional probabilities, assuming $\\alpha=1$:\n",
    "\n",
    "$$\\hat{P}(Chinese|y=yes)=\\dfrac{5+1}{8+6}=\\dfrac{6}{14}=\\dfrac{3}{7}$$\n",
    "\n",
    "$$\\hat{P}(Tokyo|y=yes)=\\hat{P}(Japan|y=yes)=\\dfrac{0+1}{8+6}=\\dfrac{1}{14}$$\n",
    "\n",
    "$$\\hat{P}(Chinese|y=no)=\\dfrac{1+1}{3+6}=\\dfrac{2}{9}$$\n",
    "\n",
    "$$\\hat{P}(Tokyo|y=yes)=\\hat{P}(Japan|y=yes)=\\dfrac{1+1}{3+6}=\\dfrac{2}{9}$$\n",
    "\n",
    "We then get,\n",
    "\n",
    "$$\\hat{P}(y=yes|d_5)\\propto 3/4 \\cdot (3/7)^3 \\cdot 1/14 \\cdot 1/14 \\approx 0.0003$$\n",
    "\n",
    "$$\\hat{P}(y=no|d_5)\\propto 1/4 \\cdot (2/9)^3 \\cdot 2/9 \\cdot 2/9 \\approx 0.0001$$\n",
    "\n",
    "Thus, the classifier assigns the test document to $c=$ China. The reason for this classification decision is that the three occurrences of the positive indicator Chinese in $d_5$ outweigh the occurrences of the two negative indicators Japan and Tokyo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "For sentiment classification, I used the popular IMDB dataset from [here](https://ai.stanford.edu/~amaas/data/sentiment/). This dataset provides a set of 25,000 highly polar movie reviews for training, and 25,000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_dir(dir_path, label):\n",
    "    \"\"\" Read all the files in the directory `dir_path` with the labels `label` and\n",
    "        return a list of tuples (text, label).\"\"\"\n",
    "    \n",
    "    files = os.listdir(dir_path)\n",
    "    data = []\n",
    "    for file in tqdm(files, file=sys.stdout):\n",
    "        with open(os.path.join(dir_path,file), 'r', encoding='utf-8') as f:\n",
    "            data.append((f.read(),label))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(task):\n",
    "    \"\"\" Load all the positive and negative examples for the training or test sets according to argument `task`,\n",
    "        and return the shuffled data.\"\"\"\n",
    "    \n",
    "    neg_dir_path = os.path.join('imdb_data', task, 'neg')\n",
    "    pos_dir_path = os.path.join('imdb_data', task, 'pos')\n",
    "    label = 0\n",
    "    neg_data = read_dir(neg_dir_path, label)\n",
    "    label = 1\n",
    "    pos_data = read_dir(pos_dir_path, label)\n",
    "    data = neg_data + pos_data\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def train(x_train, y_train, stop_words='english', ngram_range=(1, 1), max_features=None):\n",
    "    \"\"\"Create the BOW (i.e. word-count matrix) for the training set depending on input arguments\n",
    "        whether or not consider `stop_words`, `ngram_range` such as words, bigrams, etc and\n",
    "        `max_features`, which is the size of the vocabulary. Return the model and vectorizer objects.\"\"\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(input=x_train, stop_words=stop_words, ngram_range=ngram_range, min_df=10, max_df=0.9, max_features=max_features)\n",
    "    print('Vectorizing training data i.e. Creating the word count matrix...', end=' ')\n",
    "    x_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "    print('done!\\n')\n",
    "    print('Start training...')\n",
    "    model = MultinomialNB()\n",
    "    model.fit(x_train_vectorized, y_train)\n",
    "    print('Training done!')\n",
    "    print('Number of documents = {}  |  Number of features = {}'.format(x_train_vectorized.shape[0], x_train_vectorized.shape[1]))\n",
    "    return model, vectorizer\n",
    "\n",
    "\n",
    "def test(x_test, y_test, model):\n",
    "    \"\"\"Perform the prediction on the test set `x_test` and measures the accuracy based on actual labels `y_test`. \n",
    "        Return the predictions and accuracy.\"\"\"\n",
    "    \n",
    "    print('Start testing...')\n",
    "    predictions = model.predict(x_test)\n",
    "    accuracy = model.score(x_test, y_test)\n",
    "    print('done!')\n",
    "    return predictions, accuracy\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, stop_words, ngram_range, max_features):\n",
    "    \"\"\"Vectorize the data, create the model, run the train and test and measure the accuracy considering the input arquments.\"\"\"\n",
    "    \n",
    "    st_time = time.time()\n",
    "    model,vectorizer = train(x_train, y_train, stop_words, ngram_range, max_features)\n",
    "    en_time = time.time() - st_time\n",
    "    print('Training time: {:.2f} s'.format(en_time))\n",
    "    print()\n",
    "    print('Vectorizing test data...', end=' ')\n",
    "    x_test_vectorized = vectorizer.transform(x_test)\n",
    "    print('done!')\n",
    "    print('Test data shape = ', x_test_vectorized.shape)\n",
    "    predictions, accuracy = test(x_test_vectorized, y_test, model)\n",
    "    print('accuracy = {:.2f}'.format(accuracy))\n",
    "    return predictions, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the files and create our training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 18679.50it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 17331.23it/s]\n",
      "done!\n",
      "Loading test data...\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 5996.60it/s]\n",
      "100%|██████████| 12500/12500 [00:00<00:00, 13607.38it/s]\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print('Loading training data...')\n",
    "train_data = load_data('train')\n",
    "print('done!')\n",
    "print('Loading test data...')\n",
    "test_data  = load_data('test')\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the (text, label) for training and test data\n",
    "x_train, y_train = zip(*train_data)\n",
    "x_test, y_test   = zip(*test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different configurations that we can experiment with. To start it off, we train the model based on input arquments. This model removes the stop words, creates the unigrams and bigrams for all the documents and take into account the entire vocabulary. After training, the model outputs useful information such as the accuracy of the model and total training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training data i.e. Creating the word count matrix... done!\n",
      "\n",
      "Start training...\n",
      "Training done!\n",
      "Number of documents = 25000  |  Number of features = 37842\n",
      "Training time: 12.09 s\n",
      "\n",
      "Vectorizing test data... done!\n",
      "Test data shape =  (25000, 37842)\n",
      "Start testing...\n",
      "done!\n",
      "accuracy = 0.85\n"
     ]
    }
   ],
   "source": [
    "# Training the model with following parameters\n",
    "# Remove stop words\n",
    "stop_words   = 'english'\n",
    "\n",
    "# Consider uni-grams and bi-grams\n",
    "ngram_range  = (1, 2)\n",
    "\n",
    "# Take all the features into account\n",
    "max_features = None\n",
    "\n",
    "preds, acc = run(x_train, y_train, x_test, y_test, stop_words, ngram_range, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the this experiment, we only consider the unigrams (i.e. representing documents based on terms). The remaining input parameters are as before. We can see that the accuracy is decreased, which indicate that bigrams help the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training data i.e. Creating the word count matrix... done!\n",
      "\n",
      "Start training...\n",
      "Training done!\n",
      "Number of documents = 25000  |  Number of features = 18222\n",
      "Training time: 2.72 s\n",
      "\n",
      "Vectorizing test data... done!\n",
      "Test data shape =  (25000, 18222)\n",
      "Start testing...\n",
      "done!\n",
      "accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "# Another configuration\n",
    "stop_words   = 'english'\n",
    "ngram_range  = (1, 1)\n",
    "max_features = None\n",
    "preds, acc = run(x_train, y_train, x_test, y_test, stop_words, ngram_range, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this configuration, we only consider the 5000 common words in the documents, but take the bigrams into account and perform the classification task. It gives better accuracy comparing with previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training data i.e. Creating the word count matrix... done!\n",
      "\n",
      "Start training...\n",
      "Training done!\n",
      "Number of documents = 25000  |  Number of features = 5000\n",
      "Training time: 11.79 s\n",
      "\n",
      "Vectorizing test data... done!\n",
      "Test data shape =  (25000, 5000)\n",
      "Start testing...\n",
      "done!\n",
      "accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "# Another configuration\n",
    "stop_words   = 'english'\n",
    "ngram_range  = (1, 2)\n",
    "max_features = 5000\n",
    "preds, acc = run(x_train, y_train, x_test, y_test, stop_words, ngram_range, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we keep the stop words but limit the vocabulary size to 5000 and only consider unigrams. The result shows that cutting the size of the vocab lowers the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training data i.e. Creating the word count matrix... done!\n",
      "\n",
      "Start training...\n",
      "Training done!\n",
      "Number of documents = 25000  |  Number of features = 5000\n",
      "Training time: 2.95 s\n",
      "\n",
      "Vectorizing test data... done!\n",
      "Test data shape =  (25000, 5000)\n",
      "Start testing...\n",
      "done!\n",
      "accuracy = 0.83\n"
     ]
    }
   ],
   "source": [
    "# Another configuration\n",
    "stop_words   = None\n",
    "ngram_range  = (1, 1)\n",
    "max_features = 5000\n",
    "preds, acc = run(x_train, y_train, x_test, y_test, stop_words, ngram_range, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last experiment, we keep the stop words, consider both unigrams and bigrams and take into account the entire vocabulary. It gives us the best accuracy among all the configuration we experimented with. It demonstrates that keeping stop words help in better sentiment analysis, unlike some other classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing training data i.e. Creating the word count matrix... done!\n",
      "\n",
      "Start training...\n",
      "Training done!\n",
      "Number of documents = 25000  |  Number of features = 80869\n",
      "Training time: 13.53 s\n",
      "\n",
      "Vectorizing test data... done!\n",
      "Test data shape =  (25000, 80869)\n",
      "Start testing...\n",
      "done!\n",
      "accuracy = 0.86\n"
     ]
    }
   ],
   "source": [
    "# Another configuration\n",
    "stop_words   = None\n",
    "ngram_range  = (1, 2)\n",
    "max_features = None\n",
    "preds, acc = run(x_train, y_train, x_test, y_test, stop_words, ngram_range, max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other configurations that I did not experiment with. But I leave that as an exercise. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an LSTM model on the IMDB sentiment classification task\n",
    "The code below is adopted directly from [Keras website](https://keras.io/examples/imdb_lstm/). This code implements an LSTM model on the same IMDB dataset for sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes a lot of time to run on CPU, therefore I ran it on Google Colab, which took roughly around 30 minutes to train. You can see the result below:\n",
    "\n",
    "```\n",
    "Loading data...\n",
    "25000 train sequences\n",
    "25000 test sequences\n",
    "Pad sequences (samples x time)\n",
    "x_train shape: (25000, 80)\n",
    "x_test shape: (25000, 80)\n",
    "Build model...\n",
    "Train...\n",
    "Train on 25000 samples, validate on 25000 samples\n",
    "Epoch 1/15\n",
    "25000/25000 [==============================] - 137s 5ms/sample - loss: 0.4546 - accuracy: 0.7889 - val_loss: 0.3738 - val_accuracy: 0.8394\n",
    "Epoch 2/15\n",
    "25000/25000 [==============================] - 133s 5ms/sample - loss: 0.2930 - accuracy: 0.8816 - val_loss: 0.3865 - val_accuracy: 0.8281\n",
    "Epoch 3/15\n",
    "25000/25000 [==============================] - 130s 5ms/sample - loss: 0.2111 - accuracy: 0.9173 - val_loss: 0.4440 - val_accuracy: 0.8348\n",
    "Epoch 4/15\n",
    "25000/25000 [==============================] - 128s 5ms/sample - loss: 0.1484 - accuracy: 0.9449 - val_loss: 0.4976 - val_accuracy: 0.8283\n",
    "Epoch 5/15\n",
    "25000/25000 [==============================] - 127s 5ms/sample - loss: 0.1022 - accuracy: 0.9627 - val_loss: 0.6127 - val_accuracy: 0.8238\n",
    "Epoch 6/15\n",
    "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0781 - accuracy: 0.9734 - val_loss: 0.6076 - val_accuracy: 0.8173\n",
    "Epoch 7/15\n",
    "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0587 - accuracy: 0.9794 - val_loss: 0.7733 - val_accuracy: 0.8188\n",
    "Epoch 8/15\n",
    "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0430 - accuracy: 0.9860 - val_loss: 0.9582 - val_accuracy: 0.7603\n",
    "Epoch 9/15\n",
    "25000/25000 [==============================] - 125s 5ms/sample - loss: 0.0482 - accuracy: 0.9847 - val_loss: 0.7996 - val_accuracy: 0.8190\n",
    "Epoch 10/15\n",
    "25000/25000 [==============================] - 125s 5ms/sample - loss: 0.0270 - accuracy: 0.9916 - val_loss: 0.8270 - val_accuracy: 0.8106\n",
    "Epoch 11/15\n",
    "25000/25000 [==============================] - 125s 5ms/sample - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.8667 - val_accuracy: 0.8120\n",
    "Epoch 12/15\n",
    "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0176 - accuracy: 0.9944 - val_loss: 1.0325 - val_accuracy: 0.8141\n",
    "Epoch 13/15\n",
    "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0149 - accuracy: 0.9948 - val_loss: 0.9852 - val_accuracy: 0.8104\n",
    "Epoch 14/15\n",
    "25000/25000 [==============================] - 126s 5ms/sample - loss: 0.0140 - accuracy: 0.9955 - val_loss: 1.0443 - val_accuracy: 0.8127\n",
    "Epoch 15/15\n",
    "25000/25000 [==============================] - 125s 5ms/sample - loss: 0.0133 - accuracy: 0.9956 - val_loss: 1.1763 - val_accuracy: 0.8127\n",
    "25000/25000 [==============================] - 16s 656us/sample - loss: 1.1763 - accuracy: 0.8127\n",
    "Test score: 1.1762849241028726\n",
    "Test accuracy: 0.81268\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Naive Bayes with LSTM for Sentiment Analysis\n",
    "\n",
    "If we compare Naive Bayes with LSTM, we find out some interesting observations:\n",
    "\n",
    "1. Implementing Naive Bayes is very straightforward compared to LSTM.\n",
    "\n",
    "2. Training NB is extremely fast, a few seconds, whereas the implemented LSTM takes about 30 minutes on GPU. Please note that this LSTM implementation even cuts the max features into 20000.\n",
    "\n",
    "3. Number of parameters that we can alter for NB is very few unlike LSTM that we need to perform lots of fine tuning.\n",
    "\n",
    "4. Scaling Naive Bayes implementation to large datasets having millions of documents is quite easy whereas for LSTM we certainly need plenty of resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the image below, you notice that the state-of-the-art for sentiment analysis belongs to a technique that utilizes Naive Bayes bag of n-grams. See the [paper](https://www.aclweb.org/anthology/P19-2057.pdf) for all the details. This method gives the best accuracy higher than many purely deep learning methods such as BERT and LSTM+CNN.\n",
    "\n",
    "**Don't get me wrong!** I am a big fan of neural networks and deep learning techniques. The point that I am trying to make is that in many cases we may not need very complex methods for our tasks. My approach is always start off with simpler techniques and if they are not satisfactory, then move to more sophisticated ones.\n",
    "\n",
    "![](images/nb_sota.png)\n",
    "*State-of-the-art sentiment analysis on the IMDB dataset. [[Image Source](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
